# -*- coding: utf-8 -*-
"""hw_4_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LtYP_uUAsQPkFtuDdwcn4BOw4ZmHQ_CS

# Homework 4 (coding part)

## Tetris Game (SARSA with linear function approximation)

Consider the game of Tetris. The player is continually given pieces of varying shape that must be positioned and rotated, then dropped on the pieces below. Since these pieces begin to pile up, the player must try to stack them efficiently. Furthermore, if the player manages to complete a row, then that row dissapears thus freeing up more space. The objective is to keep the height as low as possible. Note that the shape of each subsequent piece is random thus making it difficult to plan ahead (non-deterministic).

We consider a smaller version of the Tetris game. The pieces are at most 2x2:

![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPsAAABaCAIAAADxULe6AAACFElEQVR4nO3csW4CMRAE0DjK//+yU0SKIiqw42Xv5r2CDssahtE1MOacHxDj890XgFIaTxaNJ4vGk0XjyaLxZNF4smg8WTSeLBpPFo0ni8aTRePJovFk0XiyaDxZNJ4sGk8WjSeLxpNF48mi8WTReLJ8vfsCdzbG2Hn7ob8SOnerzZN3PJ+VjT/r55NYe73irXZOrslK488aY8w5F14vequdk2uy0viz1j7Cgo0/dKudk2uy0vizbLyNz2LjbXwWG2/js9h4G5/Fxtv4LDbexmex8TY+i4238VlsvI3PYuNtfBYbb+Oz2Hgbn8XG2/gsNt7GZ7HxNj6Lje+28a+/IexXkm714HK3erCy8b/fyIXXcye3vdWywKwO3eqvlcanPQXe71m8Z1Z9n+PXrvXkQjR8Cjx38o77ZVWTs4238V2yqsnZxtv4LlnV5GzjbXyXrGpytvE2vktWNTnbeBvfJauanG28je+SVU3ONt7Gd8mqJmcbb+O7ZFWTs4238V2yqsnZxtv4LlnV5GzjbXyXrGpytvE2vktWNTnbeBvfJauanG28je+SVU3ONt7Gd8mqJmcbb+O7ZFWT8+tv8DvXfzp5xy2z2vF8zsenCFrx7x1k0XiyaDxZNJ4sGk8WjSeLxpNF48mi8WTReLJoPFk0niwaTxaNJ4vGk0XjyaLxZNF4smg8WTSeLBpPFo0ni8aTRePJ8g3v0+N6zqHVhgAAAABJRU5ErkJggg==)

The area where pieces are placed is 6 units wide, and the maximum working height is 2. Some examples are:

![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAd8AAABYCAIAAAA7oGrbAAAESUlEQVR4nO3cwW4jOxBD0epB/v+XO4tBvHEWQssRWcV7FsbbOHG1REowJu+677sAAGb+qT8AAOAXtDMAOKKdAcAR7QwAjmhnAHBEOwOAI9oZABzRzgDgiHYGAEe0MwA4op0BwBHtDACOaGcAcPS18+bruv7/B/+jO6EPrgIL6oAFHebxKnB3BgBHtDMAOKKdAcAR7QwAjmhnAHBEOwOAI9oZABzRzgDgiHYGAEdbfys4w+sveZ7hr7DcqBZ0/EbaHFCl74Jyd676eY7PXuFJtZqzd9FOUlSvqnn30c5VVdd13ff94FX9wfG7Z6v5kTBLfu8xO0lRvVc17z7auSogVGkyw3zATlJU71XNu492rgoIVZrMMB+guv9mHre0c1VAqNJkhvkA1f0387ilnasCQpUmM8wHqO6/mcct7VwVEKo0mWE+QHX/zTxuaeeqgFClyQzzAar7b+ZxSztXBYQqTWaYD1DdfzOPW9q5KiBUaTLDfIDq/pt53NLOVQGhSpMZ5gNU99/M45Z2rgoIVZrMMB+guv9mHre0c1VAqNJkhvkA1f0387ilnasCQpUmM8wHqO6/mcct7VwVEKo0mWE+QHX/zTxuaeeqgFClyQzzAar7b+ZxSztXBYQqTWaYD1DdfzOPW9q5KiBUaTLDfIDq/pt53NLOVQGhSpMZ5gNU99/M43brB712lbynhPvbZ/b9T/L+ozYf7OOP1KiwPuX9mRsuKNbtL+icu/PrjD38mkD1ZHZ+b8f3nqFKyq0gnHffnHbu+K1WC8+ezP4G3fm9Hd97TFpS+s47p53Hh0pFtUFVoeob5kVpSek775x2Hh8qFdUGVYWqb5gXpSWl77xz2nl8qFRUG1QVqr5hXpSWlL7zzmnn8aFSUW1QVaj6hnlRWlL6zjunnceHSkW1QVWh6hvmRWlJ6TvvnHYeHyoV1QZVhapvmBelJaXvvHPaeXyoVFQbVBWqvmFelJaUvvPOaefxoVJRbVBVqPqGeVFaUvrOO6edx4dKRbVBVaHqG+ZFaUnpO++cdh4fKhXVBlWFqm+YF6Ulpe+8c9p5fKhUVBtUFaq+YV6UlpS+885p5/GhUlFtUFWo+oZ5UVpS+s47p53Hh0pFtUFVoeob5kVpSek775x2Hh8qFdUGVYWqb5gXpSWl77xz2nl8qFRUG1QVqr5hXpSWlL7zzmnn8aFSUW1QVaj6hnlRWlL6zjunnceHSkW1QVWh6hvmRWlJ6Tvv1g967Sp5TwkPZ5/Z9z/J+4/afLCPP5JwQVXen7nhgu6QJMVh3scL+vXhTyQir8ipVA+WBf0jaQ+29bxzvtkAgEloZwBw9JlvNgK/JZyNBR2GBe2IuzMAOKKdAcBRyj/aBYBeuDsDgCPaGQAc0c4A4Ih2BgBHtDMAOKKdAcAR7QwAjmhnAHBEOwOAI9oZABzRzgDgiHYGAEe0MwA4op0BwBHtDACOaGcAcPQN67IJcIncr+4AAAAASUVORK5CYII=)

As in the regular game of tetris, rows are removed when they become full. The following picture (going left to right) shows how this happens when a piece is added:

![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAdIAAABhCAIAAADtK7cOAAAEK0lEQVR4nO3dwW7jOBAEUHsx///L3sMCxkIBMozElFj0ewffPCpIze6OMYafr9frAUDKP3cHAPgs2i5AlLYLEKXtAkRpuwBR2i5AlLYLEKXtAkRpuwBR2i5AlLYLEKXtAkRpuwBR2i5AlLYLEKXtAkRpuwBR2i5AlLYLEKXtAkT9uTsAPB6Px/P5vPJ2v8RKEdsuQJS2CxCl7QJEabsAUdouQJS2CxCl7QJEabsAUdouQNTPvqX2/iqRLwXdaOJT8EBX4IFu5q9PwbYLEKXtAkRpuwBR2i5AlLYLEKXtAkRpuwBR2i5AlLYLEHX1t9QafwLrYua7ZO5V6c1pjL33F8kaO0PMhG33vxt07vUuVzLf9Zq0fsKvrmS+6718pglt9/l8vl6vE6/XL31L5rveG3Mu4e1z9HTmu97Lx5qz7dZVnlP6jYrZMDHz9nOU1dh2ndKjitkwMfP2c5TV2Had0qOK2TAx8/ZzlNXYdp3So4rZMDHz9nOU1dh2ndKjitkwMfP2c5TV2Had0qOK2TAx8/ZzlNXYdp3So4rZMDHz9nOU1dh2ndKjitkwMfP2c5TV2Had0qOK2TAx8/ZzlNXYdp3So4rZMDHz9nOU1dh2ndKjitkwMfP2c5TV2Had0qOK2TAx8/ZzlNXYdp3So4rZMDHz9nOU1dh2ndKjitkwMfP2c5TV2Had0qOK2TAx8/ZzlNXYdp3So4rZMDHz9nOU1dh2ndKjitkwMfP2c5TV/OxIv8vl/a6LBXS6oXxg4X6959fb8fQHyriKB9p4Qu/KPP5Ai39L7cp1L76ec+O9+qnTCa/cnCvXLc2ccS7h9ZxXrtuYeVzxZ7uNfxtW/E16LuGUiq/79Oau6/5IY7U3Zh5X/NmuU/pLGiu+MXNMY7U3Zh5n263JHNNY8Y2ZYxqrvTHzONtuTeaYxopvzBzTWO2NmcfZdmsyxzRWfGPmmMZqb8w8zrZbkzmmseIbM8c0Vntj5nG23ZrMMY0V35g5prHaGzOPs+3WZI5prPjGzDGN1d6YeZxttyZzTGPFN2aOaaz2xszjbLs1mWMaK74xc0xjtTdmHmfbrckc01jxjZljGqu9MfM4225N5pjGim/MHNNY7Y2Zx9l2azLHNFZ8Y+aYxmpvzDzOtluTOaax4hszxzRWe2PmcbbdmswxjRXfmDmmsdobM4+z7dZkjmms+MbMMY3V3ph5nG23JnNMY8U3Zo5prPbGzONsuzWZYxorvjFzTGO1N2Ye97M3vMvl/a6LBXT6NoUL9//uyvz1nl8vMg/04YF+ywM9cd2/PtA/Vy7zzb/72+667hUVmT3QcRWZPdBxscwTPmQAYJy2CxB18kOGGz+44Td4oJvxQFdm2wWI0nYBotL/JxTgw9l2AaK0XYAobRcgStsFiNJ2AaK0XYAobRcgStsFiPoXHRqYSKshFpcAAAAASUVORK5CYII=)


When the working height of 2 is exceeded, the bottom row is removed and the total height is incremented:

![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAeMAAAB0CAIAAAA5Ac0gAAAFXElEQVR4nO3d0VLbSBAF0Ghr//+XvQ9UXMYCPGhamtvecx4okhjrGs1tDUoI2+12+wNAsH9WBwDgBZMaIJ1JDZDOpAZIZ1IDpDOpAdKZ1ADpTGqAdCY1QDqTGiCdSQ2QzqQGSGdSA6QzqQHSmdQA6UxqgHQmNUA6kxognUkNkM6kBkhnUgOkM6kB0pnUAOlMaoB0JjVAOpMaIN2/qwMAp9u27eOd2+22Nsn/2cxZsKcGSGdSA6QzqQHSmdQA6UxqgHQmNUA6kxognUkNkM6kBkjnexSJdv+2ruv5dr4zOKHH2FMDpDOpAdKZ1ADpTGqAdCY1QDqTGiCdSQ2QzqQGSGdSA6Sr+R5FP6UtQeFZcEITOAvc2VMDpDOpAdKZ1ADpTGqAdCY1QDqTGiCdSQ2QzqQGSGdSA6Q76+coTv60tCXflLXwJ7zNuOZz1fGETlq1HpzQk7Q+oSfuqT/yHXu7ykzmVW+vlJ9wbybzqo+9RpdV9yihcUs+VydO6m3bbrfbgbfnRTo186qPvcyxhMuLfTjzqo+9TItVV5i5dUPP3VPnL9bCzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt1Qe+qyzIqt2DnFHtFi1RVmbt3Qmue6r7D7s02uucOp1s6FJfaf8/lz6oQu5IS+mZITetZPvP0zcRWaX0OHjzv59ljgyePOfK5+a1XCmeMe/tjJYRR+Nldtjdduyft6w/vUM8ftmPkyF1/ASo7bMTPsveF9asU+ScfLWMfMsHfi3Y+OJemY+TKHLyeTOWeO2zHzzHFfcjFoyp7603E7Zr5Mx8tYx8yw5z61Yo/qeBnrmBn2/NuPsrcdP1e/0vEy1jHzGVwDurOnVuxRq/aJM8ftmBn23KdW7FEdL2MdM8OePbVij+p4GeuYGfbsqRV7VMfLWMfMsGdPrdijOl7GOmaGPXtqxR7V8TLWMTPs2VMr9qiOl7GOmWHPnlqxR3W8jHXMDHv21Io9quNlrGPmBNtfX/7yy8f/8DxvnOqyDtpTK/aoVZeTmeN2zJzgXocvf/nd48d//z1SXXm+7KkVe1THy1jHzG8v54L06LepLj5f9tSKParjZaxj5ha2bds+34V4+qPvHnA/L08vtuS1l6f6IduX7TvvDNa0/Z7v/myTiQ+nWrjWV2Xef87nz6kT+ue9TugB+1fxGOzj/cl3Ro779MglqY79Ua2z/i+9VV/Hdfz6sUVmJ3Rcx8xfenwhZ2yBXx705QPOTvW06V54Zk/8X0+Bd/VyU1z1Fcmvnqc81cgXAdc48T418K6eNpt397/vOfzMt89CUi1XfJ+aBIX3qUlw/Yh5ukX+w99bPP5F6P7B+/cfH/N0H+Ply7w+1a/yDL6KY0zqN2RSv5lGm8HHUZVw0+DDZanOe3J3PwAKnHsNCLnoAe9h7T8u/E5mqnEmNUA6dz8A0pnUAOlMaoB0JjVAOpMaIJ1JDZDOpAZIZ1IDpDOpAdKZ1ADp/gPuT/poWgOehwAAAABJRU5ErkJggg==)

Note that a lower height indicates a better performance of the agent.

An example of a state that should have a low value is:

![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJYAAACMCAIAAAARLsdIAAAD10lEQVR4nO2d23LkIAxE7a39/19mH1KhfBUga0fu4ZwHKvEYN6jdZi7OZC2lLKDMn+wBwFOwUB4slAcL5cFCebBQHiyUBwvlwUJ5sFCev8Zj67r+/MCbcIk0XSCF8mChPFgoDxbKg4XyYKE8WCgPFsqDhfJgoTxYKA8WyoOF8mChPFgoDxbKg4XyYKE8WCiPde9MLPUeEB/u+3ce6mbRP99Pp/BnZKNtrm5W304+amEpZV1XR5uom9W3n49a6JvP8yvhE92svv2QQlI4AikkhQm6pHAHKSSFCbqkcAcpJIUJuqRwBykkhQm6pHAHKSSFCbqkcAcpJIUJuqRwBykkhQm6pHAHKfwf87U8r8eq+zyvJnRyrvmdU54U1qvEUCvat3h5PuYBP+xBbPdxj0m69RGl23Rq2EL3mET7GvWxidKNt3DC1keUbryFWWnI6mvUxyZKN97CCVsfUbrxFmalIauvUR+bKN14CydsfUTpxluYlYasvkZ9bKJ04y2csPURpRtvYVYasvoa9bGJ0o23cMLWR5RuvIVZacjqa9THJko33sIJWx9RuvEWZqUhq69RH5so3XgLJ2x9ROnGW5iVhqy+Rn1sonTjLZyw9RGlG29hVhqy+hr1sYnSbVrIvTMNjPrYPKzVueZ3Ixn76iD3fCbkY7XiC7zkwUJ5ui6kEy6BQpBCebBQnoA/OoFcSKE8WCgPFsqDhfJgoTxYKA8WyoOF8mChPFgoDxbKg4XyfO5/NimS+EFp/8cPpLBNyp1z/WBhg8J3sKnz/u9gw8IGpFAeUigPKZSHFMrz/hQ+emm/3vzFxt3250KVs2KU1ln6m1NYSvkp3EG1eF+lNoXKL+u6bkUdM++XfnkKwy6kURXsPM55nqQwZraBR3sJ709hzNvczXPnsDrerW315F26U3Vedy9X4sNV197z/OsUKbzzZvmdSV3Dlv3atmwquF3wbLntmX6u+FbrsNHesz60ndcUKdzO9nwWLzfWOkZ8mZsera1oc1SHY748hcGfF1YXDxsDj/9wh9E93Ul62HbOYvnMS/vLy1R1evSkc2hd7tOz56KQwngLD2d32a+Rh2Wm3K+gbulLrcPGnj3rMV++Fg532HW+eppXHzqviMu+doc6Xv56FjIe6jlC58bDON1Jcvft9+WRhV9P7EV+iH5feJu7Tb0qDrXP+3aChQ3evxZiYYMZn5F+GaRQHlIoDymUhxTKQwrlIYXykEJ5SKE8pFAeUigPKZSHFMpDCuUhhfKQQnnen8Jhz6ditJqB9PuChfJwIZUHC+XBQnmwUB4slAcL5cFCef4BsOmkjQAM0VoAAAAASUVORK5CYII=)

Given the next piece is a solid 2x2 block, the agent will be forced to increase the total height of the pile by 2 units. Therefore it is desirable to avoid getting into states like these.

Reference: This small version of the Tetris game and the rules are from https://melax.github.io/tetris/tetris.html


### Formulation

- *State* $s$:
    
    The state $s$ contains two numbers, i.e., $s=(s_{\mathrm{gam}},s_{\mathrm{pie}})$. The number $s_{\mathrm{gam}}$ represents the game area, and the number $s_{\mathrm{pie}}$ represents the next piece. We use binary codes to encode the shape, where "1" means that there is a square in the correponding space, while "0" means that the space is empty. The 12 bits of the binary representation of $s_{\mathrm{gam}}$ encode the whole game area, where the lower 6 bits encode the lower layer of the game area and the higher 6 bits encode the higher layer. The 4 bits of the binary representation of $s_{\mathrm{pie}}$ encode the next piece, where the lower 2 bits encode the lower layer of the next piece and the higher 2 bits encode the higher layer. Take the following state as an example:
    
    ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJYAAACMCAIAAAARLsdIAAAD10lEQVR4nO2d23LkIAxE7a39/19mH1KhfBUga0fu4ZwHKvEYN6jdZi7OZC2lLKDMn+wBwFOwUB4slAcL5cFCebBQHiyUBwvlwUJ5sFCev8Zj67r+/MCbcIk0XSCF8mChPFgoDxbKg4XyYKE8WCgPFsqDhfJgoTxYKA8WyoOF8mChPFgoDxbKg4XyYKE8WCiPde9MLPUeEB/u+3ce6mbRP99Pp/BnZKNtrm5W304+amEpZV1XR5uom9W3n49a6JvP8yvhE92svv2QQlI4AikkhQm6pHAHKSSFCbqkcAcpJIUJuqRwBykkhQm6pHAHKSSFCbqkcAcpJIUJuqRwBykkhQm6pHAHKfwf87U8r8eq+zyvJnRyrvmdU54U1qvEUCvat3h5PuYBP+xBbPdxj0m69RGl23Rq2EL3mET7GvWxidKNt3DC1keUbryFWWnI6mvUxyZKN97CCVsfUbrxFmalIauvUR+bKN14CydsfUTpxluYlYasvkZ9bKJ04y2csPURpRtvYVYasvoa9bGJ0o23cMLWR5RuvIVZacjqa9THJko33sIJWx9RuvEWZqUhq69RH5so3XgLJ2x9ROnGW5iVhqy+Rn1sonTjLZyw9RGlG29hVhqy+hr1sYnSbVrIvTMNjPrYPKzVueZ3Ixn76iD3fCbkY7XiC7zkwUJ5ui6kEy6BQpBCebBQnoA/OoFcSKE8WCgPFsqDhfJgoTxYKA8WyoOF8mChPFgoDxbKg4XyfO5/NimS+EFp/8cPpLBNyp1z/WBhg8J3sKnz/u9gw8IGpFAeUigPKZSHFMrz/hQ+emm/3vzFxt3250KVs2KU1ln6m1NYSvkp3EG1eF+lNoXKL+u6bkUdM++XfnkKwy6kURXsPM55nqQwZraBR3sJ709hzNvczXPnsDrerW315F26U3Vedy9X4sNV197z/OsUKbzzZvmdSV3Dlv3atmwquF3wbLntmX6u+FbrsNHesz60ndcUKdzO9nwWLzfWOkZ8mZsera1oc1SHY748hcGfF1YXDxsDj/9wh9E93Ul62HbOYvnMS/vLy1R1evSkc2hd7tOz56KQwngLD2d32a+Rh2Wm3K+gbulLrcPGnj3rMV++Fg532HW+eppXHzqviMu+doc6Xv56FjIe6jlC58bDON1Jcvft9+WRhV9P7EV+iH5feJu7Tb0qDrXP+3aChQ3evxZiYYMZn5F+GaRQHlIoDymUhxTKQwrlIYXykEJ5SKE8pFAeUigPKZSHFMpDCuUhhfKQQnnen8Jhz6ditJqB9PuChfJwIZUHC+XBQnmwUB4slAcL5cFCef4BsOmkjQAM0VoAAAAASUVORK5CYII=)
    
    In the above state, the binary representation of $s_{\mathrm{gam}}$ should be 010111 111101, whose decimal number is $1533$;
    the binary representation of $s_{\mathrm{pie}}$ should be 1111, whose decimal number is $15$. Hence, $s=(1533, 15)$.
    
    Note that two different $s_{\mathrm{pie}}$ could have the same shape after rotation. For example, there are four possible $s_{\mathrm{pie}}$ that can represent a single-square piece, i.e., the binary number 0001, 0010, 1000, or 0100.


- *Action $a$*:
    
    The action $a$ also contains two numbers, i.e., $a=(a_{\mathrm{pos}},a_{\mathrm{rot}})$. The number $a_{\mathrm{pos}}\in\{0,1,2,3,4\}$ represents the position you want to put the next piece down, counting from right to left. The number $a_{\mathrm{rot}}\in\{0,1,2,3\}$ represents the clockwise rotation of the next piece. $0$ means no rotation.
    
    Note: Taking the same action on different representations $s_{\mathrm{pie}}$ of the same shape could have different results. For example, $s_{\mathrm{pie}}=0001$ is different from $s_{\mathrm{pie}}=0010$ when taking the action $a=(0, 0)$. For $s_{\mathrm{pie}}=0001$, $a_{\mathrm{pos}}=0$ means that the single-sqaure piece will drop down at the rightmost position, but for $s_{\mathrm{pie}}=0010$, $a_{\mathrm{pos}}=0$ means that the single-sqaure piece will drop down one space left.
    

- *Reward $r(s,a)$*:
    
    If the working height of 2 is exceeded by one, i.e., the total height is incremented by one, then a reward $-1$ occurs;
    
    If the working height of 2 is exceeded by two, i.e., the total height is incremented by two, then a reward $-2$ occurs.
    

- *Objective*: Maximize the expected total discounted reward:

    $$\mathbb{E} \left[\sum_{t=0}^{\infty} \gamma^t r(s_t,a_t)\right]$$
    
    where $\gamma=0.9$ denotes the discount factor.
"""

# Upload the file "tetris_env.py"
try:
    from google.colab import files
    uploaded = files.upload()
except ImportError as e:
    pass

# Import packages. Run this cell.

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import matplotlib.patches as mpatches
from tetris_env import TetrisEnv
env = TetrisEnv(seed=0)

"""**(1).** SARSA with Linear Function Approximation (30 pts)

In this part, we will use SARSA with linear function approximation to play the Tetris game.

Please complete your agent (the python class `LFAAgent`) to implement the SARSA with linear function approximation algorithm.

**Note**:

This class will be initialized at the beginning of the test code.

The function `select_action` in the class will be called each step in the test code and returns an action for the agent to take.
The input `state_game` and `state_piece` are $s_{\mathrm{gam}}$ and $s_{\mathrm{pie}}$ defined in the **Formulation** part, respectively;
The output `action_pos` and `action_rot` are $a_{\mathrm{pos}}$ and $a_{\mathrm{rot}}$ defined in the **Formulation** part, respectively.

The function `train` in the class will be called each step after the agent takes an action, receives a reward, and observes the next state. It is used for the update of the weights.
- The input `pre_state_game` and `pre_state_piece` is the state in the previous step.
- The input `pre_action_pos` and `pre_action_rot` is the action in the previous step.
- The input `pre_reward` is the reward received in the previous step.
- The input `cur_state_game` and `cur_state_piece` is the current state.
- The input `cur_action_pos` and `cur_action_rot` is the action that the agent takes in the current step.
- The input `done=0` means that the current episode does not terminate, and `done=1` means that the current episode terminates. We set the length of each episode to be 100.

You can define any function in the class if you need, such as a function that implements $\phi(s,a)$. You may also use the `rotate` function given in the class.

"""

class LFAAgent:

    def __init__(self, seed):
        """
        initialize the coefficients theta and set hyper-parameters.
        """
        # The following are recommended hyper-parameters.

        # Initial learning rate: 0.01
        # Learning rate decay for each episode: 0.995
        # Minimum learning rate: 0.001
        # Initial epsilon for exploration: 0.5
        # Epsilon decay for each episode: 0.98

        self.gamma = 0.9  # Discount factor
        self.theta = np.zeros((640,))  # The weight vector to be learned
        self.learning_rate = 0.01  # Learning rate.
        self.learning_rate_decay = 0.995  # You may decay the learning rate as the training proceeds.
        self.min_learning_rate = 0.001
        self.epsilon = 0.5  # For the epsilon-greedy exploration.
        self.epsilon_decay = 0.98  # You may decay the epsilon as the training proceeds.
        if seed is None:
            self.rng = np.random.default_rng()
        else:
            self.rng = np.random.default_rng(seed)

    def select_action(self, state_game, state_piece):
        """
        This function returns an action for the agent to take.
        Args:
            state_game: s_{gam}, the state of the game area in the current step
            state_piece: s_{pie}, the state of the next piece in the current step
        Returns:
            action_pos: a_{pos}, the position where the agent will put the next piece down
            action_rot: a_{rot}, clockwise rotation of the piece before putting the piece down
        """

        # Please complete codes for choosing an action given the current state
        """
        Hint: You may use epsilon-greedy for exploration.
        With probability self.epsilon, choose an action uniformly at random;
        Otherwise, choose a greedy action based on the approximated Q values.
        Recall that the Q values are aprroximated by the inner product of the weight vector (self.theta) and the feature vector (self.phi).
        """
        ### BEGIN SOLUTION
        # YOUR CODE HERE
        if self.rng.random()<self.epsilon:
            action_pos=self.rng.choice([0,1,2,3,4])
            action_rot=self.rng.choice([0,1,2,3])
        else:
            q_fun=[]
            actions=[(a_pos,a_rot) for a_pos in range(5) for a_rot in range(4)]
            for action_pos, action_rot in actions:
                phifun=self.phi(state_game, state_piece, action_pos, action_rot)
                q_fun.append(np.dot(self.theta,phifun))
                best_index=np.argmax(q_fun)
                action_pos, action_rot=actions[best_index]
        return action_pos, action_rot
        # raise NotImplementedError()
        ### END SOLUTION
        return action_pos, action_rot

    def train(self, pre_state_game, pre_state_piece, pre_action_pos, pre_action_rot, pre_reward,
              cur_state_game, cur_state_piece, cur_action_pos, cur_action_rot, done):
        """
        This function is used for the update of the Q table
        Args:
            - pre_state_game: the state of the game area in the previous step
            - pre_state_piece: the state of the next piece in the previous step
            - pre_action_pos: the position where the agent puts the next piece down in the previous step
            - pre_action_rot: clockwise rotation of the piece before putting the piece down in the previous step
            - pre_reward: the reward received in the previous step.
            - cur_state_game: the state of the game area in the current step
            - cur_state_piece: the state of the next piece in the current step
            - cur_action_pos: the position where the agent puts the next piece down in the current step
            - cur_action_rot: clockwise rotation of the piece before putting the piece down in the current step
            - `done=0` means that the current episode does not terminate;
              `done=1` means that the current episode terminates.
              We set the length of each episode to be 100.
        """

        # Please complete codes for updating the weight vector self.theta
        """
        Hint: You may use the feature function self.phi
              You may use the discount factor self.gamma (=0.9)
        """
        ### BEGIN SOLUTION
        # YOUR CODE HERE
        cur_a_pos, cur_a_rot = self.select_action(cur_state_game,cur_state_piece)
        pre_phi=self.phi(pre_state_game, pre_state_piece, pre_action_pos, pre_action_rot)
        cur_phi=self.phi(cur_state_game, cur_state_piece, cur_a_pos, cur_a_rot)
        qfun_current=np.dot(self.theta, pre_phi)
        # if done!=0:
        qfun_next=np.dot(self.theta,cur_phi)
        #else:
        #    qfun_next=0
        td_target=pre_reward + self.gamma* qfun_next
        td_error= td_target - qfun_current
        self.theta = self.theta + self.learning_rate* td_error * pre_phi

        # raise NotImplementedError()
        ### END SOLUTION

        if done != 0:
            self.learning_rate = self.learning_rate * self.learning_rate_decay
            if self.learning_rate < self.min_learning_rate:
                self.learning_rate = self.min_learning_rate
            self.epsilon = self.epsilon * self.epsilon_decay

    @staticmethod
    def rotate(p, action_rot):
        """
        Rotate the piece `p` clockwise.
        Args:
            - p: the piece
            - action_rot: clockwise rotation of the piece.
                          action_rot = 0, 1, 2, or 3.
                          0 means no rotation.
        Returns:
            - a new piece after the rotation
        """
        while action_rot > 0:
            q = p >> 2
            p = (2 if p & 1 != 0 else 0) + (2 << 2 if p & 2 != 0 else 0) + (
                1 << 2 if q & 2 != 0 else 0) + (1 if q & 1 != 0 else 0)
            action_rot = action_rot - 1
        if p % (1 << 2) == 0:
            p >>= 2
        return p

    # For your reference, the following function is an example of the feature vector \phi(s,a)
    # You can directly use this function as \phi(s,a), or you can design your own.
    def phi(self, state_game, state_piece, action_pos, action_rot):
        """
        Implement the feature function phi(s, a)
        Args:
            state_game: s_{gam}, the state of the game area in the current step
            state_piece: s_{pie}, the state of the next piece in the current step
            action_pos: a_{pos}, the position where the agent puts the next piece down in the current step
            action_rot: a_{rot}, clockwise rotation of the piece before putting the piece down in the current step
        Returns:
            feature_vec: feature vector
        """
        feature_vec = np.zeros((640,))
        feature_s_vec = np.zeros((8,))
        h_row = np.unpackbits(np.array([state_game >> 6], dtype=np.uint8))
        l_row = np.unpackbits(np.array([state_game & 63], dtype=np.uint8))
        heights = h_row.astype(int) * 2 + (l_row - h_row == 1).astype(int)
        feature_s_vec[0] = np.max(heights)  # the height of the highest column
        feature_s_vec[1] = np.sum(h_row.astype(int) - l_row.astype(int) == 1)  # holes
        wells = 0
        for i in range(2, 8):
            if (i == 2 or heights[i] - heights[i - 1] < 0) and (i == 7 or heights[i + 1] - heights[i] > 0):
                wells += 1
        feature_s_vec[2] = wells  # wells

        for i in range(3, 8):
            feature_s_vec[i] = heights[i] - heights[i - 1]  # differences in height between neighboring columns

        piece_rotated = self.rotate(state_piece, action_rot)

        action = action_pos * 16 + piece_rotated
        feature_vec[action * 8:(action + 1) * 8] = feature_s_vec

        return feature_vec

    # You can use this function to visualize the game
    @staticmethod
    def visualize(env, action_rot, action_pos):
        """
        Visualize the game
        Args:
            env: Tetris environment object
            action_rot: a_{rot}, clockwise rotation of the piece before putting the piece down in the current step
            action_pos: a_{pos}, the position where the agent puts the next piece down in the current step
        """

        # borderlines of the plot
        xmin = 0
        xmax = 6
        ymin = 0
        ymax = 6

        # start plotting the game
        state_game, _ = env.get_state()

        upper_row = bin(state_game >> 6)
        upper_row = upper_row[2:].zfill(6)

        lower_row = bin(state_game & 63)
        lower_row = lower_row[2:].zfill(6)

        fig, ax = plt.subplots()

        y_lower_left_corner = 0
        x_lower_left_corner = 0

        for i in range(6):
            s = lower_row[i]
            if s == '1':
                ax.add_patch(Rectangle((x_lower_left_corner, y_lower_left_corner), 1, 1,color="black"))
            x_lower_left_corner +=1

        y_lower_left_corner += 1
        x_lower_left_corner = 0
        for i in range(6):
            s = upper_row[i]
            if s == '1':
                ax.add_patch(Rectangle((x_lower_left_corner, y_lower_left_corner), 1, 1,color="black"))
            x_lower_left_corner +=1

        # start plotting the piece
        y_lower_left_corner += 1
        state_piece = env.rotate(env.piece,action_rot) << action_pos

        upper_row_piece = bin(state_piece >> 6)
        upper_row_piece = upper_row_piece [2:].zfill(6)

        lower_row_piece = bin(state_piece & 63)
        lower_row_piece = lower_row_piece [2:].zfill(6)


        x_lower_left_corner = 0
        for i in range(6):
            s = lower_row_piece[i]
            if s == '1':
                ax.add_patch(Rectangle((x_lower_left_corner, y_lower_left_corner), 1, 1,color="red"))
            x_lower_left_corner +=1

        x_lower_left_corner = 0
        y_lower_left_corner += 1

        for i in range(6):
            s = upper_row_piece[i]
            if s == '1':
                ax.add_patch(Rectangle((x_lower_left_corner, y_lower_left_corner), 1, 1,color="red"))
            x_lower_left_corner +=1


        ax.set(xlim=(xmin, xmax), ylim=(ymin, ymax))
        plt.xlabel("X - position")
        plt.ylabel("Height")
        plt.title("Tetris Grid")
        plt.grid()
        black_patch = mpatches.Patch(color='black', label='Game Area')
        red_patch = mpatches.Patch(color='red', label='Rotated Piece')
        plt.legend(handles=[red_patch, black_patch])
        plt.show()

# The code in this cell is provided for debugging and is also a sample test.
# The following code will train and run your agent in the Teris environment for 500 episodes.
# The length of each episode is set to be 100.
# We will print your agent's first 10 actions during the last episode and the total reward averaged over the last 100 episodes.
# You can check whether your policy is good by this information.
np.random.seed(0)
agent = LFAAgent(seed=0)
print("Your actions during the last episode:")
total_reward = 0.0
num_ep = 500

for ep in range(num_ep):
    state_game, state_piece = env.reset()
    pre_state_game = None
    pre_state_piece = None
    pre_action_pos = None
    pre_action_rot = None
    pre_reward = None
    for step in range(100):
        action_pos, action_rot = agent.select_action(state_game, state_piece)

        if ep == num_ep - 1 and step < 10:
            print('step %d: ' % (step + 1))
            print('state of the game area:')
            print(format(state_game >> 6, 'b').zfill(6))
            print(format(state_game & 63, 'b').zfill(6))
            print('next piece:')
            print(format(state_piece >> 2, 'b').zfill(2))
            print(format(state_piece & 3, 'b').zfill(2))
            print('actions:')
            print('position=', action_pos, 'rotation=', action_rot)
            agent.visualize(env, action_rot, action_pos)

        next_state_game, next_state_piece, reward = env.step(action_pos, action_rot)

        if 1 <= step < 99:
            agent.train(pre_state_game, pre_state_piece, pre_action_pos, pre_action_rot, pre_reward,
                        state_game, state_piece, action_pos, action_rot, 0)
        elif step == 99:
            agent.train(pre_state_game, pre_state_piece, pre_action_pos, pre_action_rot, pre_reward,
                        state_game, state_piece, action_pos, action_rot, 1)

        if num_ep - ep <= 100:
            total_reward = total_reward + reward

        pre_state_game = state_game
        pre_state_piece = state_piece
        pre_action_pos = action_pos
        pre_action_rot = action_rot
        pre_reward = reward
        state_game = next_state_game
        state_piece = next_state_piece

total_reward = total_reward / 100
print("")
print("Your total reward averaged over the last %d episodes:\n%.3f" % (100, total_reward))

# Sample test
# Check if the total reward is larger than -15
assert total_reward >= -15, "Sample test, average total reward is less than -15."